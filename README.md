# Seed Generator 3.0

Gerador de mnemonics BIP39 otimizado com processamento paralelo usando GPU (CUDA) e multiprocessamento.

## üìã Descri√ß√£o

Este script gera mnemonics BIP39 aleat√≥rios e busca por aqueles que atendem a um crit√©rio de contagem de caracteres. Utiliza processamento paralelo com m√∫ltiplos processos e aproveitamento de GPU quando dispon√≠vel para m√°xima performance.

## üöÄ Caracter√≠sticas

- **Processamento GPU**: Utiliza CUDA quando dispon√≠vel para gera√ß√£o r√°pida de entropia
- **Multiprocessamento**: M√∫ltiplos workers paralelos para gera√ß√£o de mnemonics
- **Fila otimizada**: Sistema de fila com ajuste din√¢mico de batch size
- **Sistema de Seguran√ßa**: Limita automaticamente o uso de CPU e GPU a 80% para proteger o hardware
- **Monitoramento em Tempo Real**: Exibe uso de CPU e GPU durante a execu√ß√£o
- **Throttling Inteligente**: Reduz automaticamente a carga quando os limites s√£o atingidos
- **Interface colorida**: Sa√≠da colorida no terminal usando colorama
- **Logging**: Salva resultados em arquivo de log
- **Interrup√ß√£o segura**: Pressione 'q' e Enter para parar o script graciosamente

## üì¶ Requisitos

- Python 3.8 ou superior
- **Depend√™ncias obrigat√≥rias** (instaladas automaticamente):
  - `mnemonic>=0.20`: Gera√ß√£o de mnemonics BIP39
  - `colorama>=0.4.6`: Cores no terminal
  - `psutil>=5.9.0`: Monitoramento de CPU e recursos do sistema
- **Depend√™ncias opcionais**:
  - `pynvml>=11.5.0`: Monitoramento avan√ßado de GPU NVIDIA (recomendado para GPUs NVIDIA)
- **Acelera√ß√£o por GPU** (opcional):
  - NVIDIA (CUDA): Driver NVIDIA e PyTorch com CUDA
  - AMD/Intel (Windows): `torch-directml`

## üîß Instala√ß√£o

1. Clone o reposit√≥rio:
```bash
git clone <url-do-repositorio>
cd seed-generator-3.0
```

2. Instale as depend√™ncias base:
```bash
pip install -r requirements.txt
```

3. (Opcional) Para monitoramento avan√ßado de GPU NVIDIA:
```bash
pip install pynvml
```

**Nota sobre o aviso do pip**: Se voc√™ receber um aviso sobre scripts do pip n√£o estarem no PATH, isso n√£o afeta o funcionamento do programa. √â apenas informativo. Se quiser corrigir (opcional), adicione `C:\Users\[SeuUsuario]\AppData\Roaming\Python\Python313\Scripts` ao PATH do Windows.

### Instala√ß√£o r√°pida via scripts (Windows/PowerShell)

Execute um dos scripts abaixo a partir do diret√≥rio do projeto:

- CPU (sem GPU):
```powershell
PowerShell -ExecutionPolicy Bypass -File .\scripts\install_cpu.ps1
```
- NVIDIA CUDA:
```powershell
PowerShell -ExecutionPolicy Bypass -File .\scripts\install_cuda.ps1
```

### Instala√ß√£o do PyTorch para GPU (opcional)

Escolha UMA op√ß√£o conforme seu hardware:

#### Op√ß√£o A) NVIDIA (CUDA)

Verifique o driver:
```bash
nvidia-smi
```

Instale o PyTorch com CUDA (selecione a vers√£o):
```bash
pip uninstall -y torch torchvision torchaudio
pip install --upgrade --index-url https://download.pytorch.org/whl/cu121 torch  # CUDA 12.1
# ou
pip install --upgrade --index-url https://download.pytorch.org/whl/cu118 torch  # CUDA 11.8
```

#### Op√ß√£o B) AMD/Intel (Windows - DirectML)
```bash
pip uninstall -y torch torchvision torchaudio
pip install torch-directml
```

### Valida√ß√£o da instala√ß√£o
```bash
python -c "import torch; print('torch:', torch.__version__); print('cuda available:', torch.cuda.is_available()); import importlib; print('directml available:', importlib.util.find_spec('torch_directml') is not None)"
```

## üìñ Uso

### Uso B√°sico

Execute o script com os par√¢metros padr√£o:
```bash
python gpuseed3.py
```

### Par√¢metros Dispon√≠veis

- `--threshold`: Limite de caracteres para filtrar mnemonics (padr√£o: 45)
- `--count`: N√∫mero de mnemonics a encontrar por contagem de caracteres (padr√£o: 5)
- `--logfile`: Nome do arquivo de log (padr√£o: mnemonics_log.txt)
- `--batch-size`: Tamanho do batch para processamento GPU (padr√£o: 2048)

### Exemplos

Buscar mnemonics com 40 caracteres ou menos:
```bash
python gpuseed3.py --threshold 40
```

Encontrar 10 mnemonics por contagem de caracteres:
```bash
python gpuseed3.py --count 10
```

Usar batch size maior para melhor performance (GPU):
```bash
python gpuseed3.py --batch-size 4096
```

Combinar par√¢metros:
```bash
python gpuseed3.py --threshold 42 --count 3 --logfile resultados.txt --batch-size 4096
```

### Parar o Script

Para interromper o script de forma segura, pressione `q` e depois `Enter`.

## üìä Sa√≠da

O script exibe no console:
- Mnemonics encontrados que atendem ao crit√©rio
- Contagem total de caracteres
- N√∫mero de itera√ß√µes processadas
- Tempo decorrido
- Estat√≠sticas de processamento (mnemonics/s)
- **Uso de CPU e GPU em tempo real**
- **Status de throttling** quando o sistema de seguran√ßa est√° ativo

Exemplo de sa√≠da:
```
Processed: 1,234,567 (12,345/s) | CPU: 75.2% | GPU: 78.5%
```

Quando o throttling est√° ativo:
```
Processed: 1,234,567 (12,345/s) | CPU: 85.3% | GPU: 82.1% [THROTTLE: CPU=0.94, GPU=0.97]
```

Os resultados tamb√©m s√£o salvos no arquivo de log especificado.

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

Voc√™ pode ajustar as constantes no in√≠cio do arquivo `gpuseed3.py`:

- `NUM_STREAMS`: N√∫mero de streams CUDA para processamento paralelo (padr√£o: 4)
- `QUEUE_MAX_SIZE`: Tamanho m√°ximo da fila (padr√£o: 50000)
- `WORKER_DELAY`: Delay base nos workers (padr√£o: 0.01)
- `QUEUE_WARNING_THRESHOLD`: Limite para desacelerar workers (padr√£o: 0.9)
- `BATCH_SIZE_MIN`: Tamanho m√≠nimo do batch (padr√£o: 512)
- `MAX_USAGE_PERCENT`: Limite m√°ximo de uso de CPU/GPU para o sistema de seguran√ßa (padr√£o: 0.80 = 80%)
- `MONITOR_INTERVAL`: Intervalo de verifica√ß√£o de recursos em segundos (padr√£o: 0.5)
- `THROTTLE_DELAY_BASE`: Delay base quando throttling est√° ativo (padr√£o: 0.05)

### Sistema de Seguran√ßa

O programa inclui um sistema de seguran√ßa que monitora continuamente o uso de CPU e GPU. Quando o uso excede 80% (configur√°vel via `MAX_USAGE_PERCENT`), o sistema automaticamente:

- Reduz o tamanho dos batches processados
- Diminui o n√∫mero de streams CUDA (se aplic√°vel)
- Adiciona delays entre opera√ß√µes
- Ajusta dinamicamente a carga para manter o uso abaixo do limite

Isso protege seu hardware contra sobrecarga e permite que o sistema continue responsivo durante a execu√ß√£o.

## üêõ Troubleshooting

### PyTorch n√£o detecta GPU

1. Verifique se o CUDA est√° instalado:
```bash
nvcc --version
```

2. Verifique se o PyTorch foi instalado com suporte CUDA:
```python
python -c "import torch; print(torch.cuda.is_available())"
```

3. Se retornar `False`, reinstale o PyTorch com suporte CUDA apropriado.

### Erro ao importar mnemonic

Certifique-se de que a biblioteca foi instalada corretamente:
```bash
pip install mnemonic
```

### Performance baixa

- Aumente o `--batch-size` se tiver GPU dispon√≠vel
- Verifique se est√° usando GPU (o script informa no in√≠cio)
- Ajuste o n√∫mero de processos conforme seu hardware
- O sistema de seguran√ßa pode estar limitando a performance se o uso estiver pr√≥ximo de 80%

### Monitoramento de recursos n√£o funciona

- **CPU**: Certifique-se de que `psutil` est√° instalado (`pip install psutil`)
- **GPU NVIDIA**: Para monitoramento preciso, instale `pynvml` (`pip install pynvml`)
- Sem `pynvml`, o sistema usa estimativas baseadas em mem√≥ria CUDA (menos preciso)
- GPUs DirectML (AMD/Intel) n√£o t√™m monitoramento direto dispon√≠vel

## üìù Notas

- O script usa multiprocessamento, ent√£o aproveita todos os cores dispon√≠veis
- A fila ajusta dinamicamente o batch size para evitar sobrecarga
- O sistema de seguran√ßa monitora e limita o uso de recursos automaticamente
- Os resultados s√£o salvos incrementalmente no arquivo de log
- Mnemonics duplicados s√£o automaticamente ignorados
- O monitoramento de recursos roda em um processo separado para n√£o impactar a performance
- O sistema √© tolerante a falhas: se o monitoramento falhar, o throttling √© desabilitado para n√£o interromper o trabalho

## üìÑ Licen√ßa

[Especifique a licen√ßa do seu projeto aqui]

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou pull requests.

